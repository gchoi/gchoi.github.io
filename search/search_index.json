{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Alex Choi's Blog Contact Info: cinema4dr12@gmail.com","title":"Home"},{"location":"#welcome-to-alex-chois-blog","text":"Contact Info: cinema4dr12@gmail.com","title":"Welcome to Alex Choi's Blog"},{"location":"about/","text":"Who Am I? Geol Choi (ph.D) / \ucd5c\uac78 / Alex Choi Data Scientist / Deep Learning Professional / Research Scientist Contacts Email: cinema4dr12@gmail.com Tistory Blog: cinema4dr12.tistory.com GitHub: github.com/gchoi LinkedIn: linkedin.com/in/gchoi YouTube: youtube.com/gchoi Twitter: twitter.com/cinema4dr12 Work Experiences Vision Solution Engineer, COGNEX (2017.12 ~ Present) Data Scientist & Software Engineer, ALPINION MEDICAL SYSTEMS (2014.06 ~ 2017.11) Computer Graphics Researcher, CJ POWERCAST (2012.04 ~ 2014.03) Computer Vision & VR/AR Researcher, ETRI (2010.04 ~ 2012.03) Research Experiences Computer Vision Researcher@ Mixed and Augmented Reality Solutions Team, COOPERATION SYSTEMS Dept., Fraunhofer FIT (2010.07 ~ 2010.09) Computational Mechanics Researcher@Research Center for Ubiquitous MEMS and Micro Engineering, National Institute of Advanced Industrial Science and Technology (2006.03 ~ 2006.08) Education Nano Degree R Programming, Coursera (Johns Hopkins Univ.), 2014 Deep Neural Networks with PyTorch, Coursera (IBM), 2020 Ph.D, Mechanical Engineering Dept. from KAIST, 2010 BS, Mechanical Engineering Dept. from Chung-Ang Univ., 2003 Interests General Machine Learning & Deep Learning Convolutional Neural Networks (CNN) Deep Generative Models (GANs) Computer Vision / Machine Vision / Visual Recognition Data Science / Data Analytics / Data Visualization Computer Graphics Realtime Web Applications Scientific Computing / Parallel Computing Languages Korean: Native English: Proficiency German: Intermediate Japanese: Basic Chinese: Novice(HSK Level3) Skills Machine & Deep Learning Related Theories and Algorithms: Probability, Statistics, Optimization, etc. Multi-layer Perceptron Convolutional Neural Networks (CNN) Recurrent Neural Networks (RNN) Generative Models (GANs, VAEs) Deep Learning Frameworks (Order of Frequently Used Frameworks) PyTorch TensorFlow / Keras CNTK MXNet with Gluon Data Science Python(pands)/R SQL/NoSQL Data Wrangling Data Analytics Data Mining Data Visualization (matplotlib, ggplot, plotly, bokeh, etc.) Database (Order of Frequently Used Tools) MongoDB PostresSQL MySQL Firebird Programming Languages (Order of Skillful Languages) C/C++ Python R JavaScript C# Matlab Java Computer Vision General Image Processing Algorithms (filters, etc.) Conventional & Deep Learning based CV Theories & Algorithms Object Detection Algorithms: YOLO, SSD, R-CNN, Fast R-CNN, Faster R-CNN, Selective Search, FPN, etc. Tools: Detectron2, TensorFlow Object Detection Module Segmentation: Mask R-CNN, FCN, U-Net, etc. Mixed Reality / Augmented reality OpenCV (C++ / Python / C#) Cloud Computing Amazon Web Services (AWS) Google Cloud Platform (GCP) Google Colab (Social Coding) Web Development Basic Web Document Concepts: HTML5 / CSS / JavaScript Server-side: Node.jS, PHP Client-side: ReactJS, AngularJS, jQuery Other Web Technologies: WebGL / WebRTC / WebSocket Web Frameworks: React Scientific Computing CUDA Parallel Computing Realtime Fluid Simulation Finite Element Analysis (FEA) Boundary Element Analaysis (BEA) Computer Graphics General Computer Graphics Theroies & Algorithms Rendering Theories: Global Illumination, Path Tracing, Ray Tracing Particle Simulation Tools: Maxon Cinema 4D, Adobe Photoshop & AfterEffects, NextLimit RealFlow, SideEffects Houdini, E-on Software Vue Publications Doctoral Thesis Isogeometric Analysis Based on Boundary Element Method , 2010, Thesis Ph.D., KAIST International Conferences Application of Isogeometric Analysis to Shape Optimal Design of Thermal Problems, 8th World Congress on Structural and Multidisciplinary Optimization, Lisbon, Portugal, 2009. Structural shape optimization based on isogeometric analysis with control weights, International Congress of Theoretical and Applied Mechanics, Adelaide, Australia, 2008. Structural shape optimization using shape design sensitivity of NURBS control weights, CJK-OSM 5, Cheju, Korea, 2008. Structural shape optimization using extended finite element method on boundary representation by NURBS, 7th World Congress on Structural and Multidisciplinary Optimization, Seoul, Korea, 2007. Imposing essential and natural boundary conditions on geometric boundaries for X-FEM, CJK-OSM 4, Kumming, China, 2006. Student Symposium Bone growth simulation based on topology optimization, KAIST-Michigan Univ. Joint Symposium, University of Michigan, Ann Arbor, USA, 2006. Articles Realtime Fluid Simulation Using Massively Parallel Computing with CUDA, 2013 , CJ POWERCAST Research Aritcles","title":"About"},{"location":"about/#who-am-i","text":"Geol Choi (ph.D) / \ucd5c\uac78 / Alex Choi Data Scientist / Deep Learning Professional / Research Scientist","title":"Who Am I?"},{"location":"about/#contacts","text":"Email: cinema4dr12@gmail.com Tistory Blog: cinema4dr12.tistory.com GitHub: github.com/gchoi LinkedIn: linkedin.com/in/gchoi YouTube: youtube.com/gchoi Twitter: twitter.com/cinema4dr12","title":"Contacts"},{"location":"about/#work-experiences","text":"Vision Solution Engineer, COGNEX (2017.12 ~ Present) Data Scientist & Software Engineer, ALPINION MEDICAL SYSTEMS (2014.06 ~ 2017.11) Computer Graphics Researcher, CJ POWERCAST (2012.04 ~ 2014.03) Computer Vision & VR/AR Researcher, ETRI (2010.04 ~ 2012.03)","title":"Work Experiences"},{"location":"about/#research-experiences","text":"Computer Vision Researcher@ Mixed and Augmented Reality Solutions Team, COOPERATION SYSTEMS Dept., Fraunhofer FIT (2010.07 ~ 2010.09) Computational Mechanics Researcher@Research Center for Ubiquitous MEMS and Micro Engineering, National Institute of Advanced Industrial Science and Technology (2006.03 ~ 2006.08)","title":"Research Experiences"},{"location":"about/#education","text":"Nano Degree R Programming, Coursera (Johns Hopkins Univ.), 2014 Deep Neural Networks with PyTorch, Coursera (IBM), 2020 Ph.D, Mechanical Engineering Dept. from KAIST, 2010 BS, Mechanical Engineering Dept. from Chung-Ang Univ., 2003","title":"Education"},{"location":"about/#interests","text":"General Machine Learning & Deep Learning Convolutional Neural Networks (CNN) Deep Generative Models (GANs) Computer Vision / Machine Vision / Visual Recognition Data Science / Data Analytics / Data Visualization Computer Graphics Realtime Web Applications Scientific Computing / Parallel Computing","title":"Interests"},{"location":"about/#languages","text":"Korean: Native English: Proficiency German: Intermediate Japanese: Basic Chinese: Novice(HSK Level3)","title":"Languages"},{"location":"about/#skills","text":"","title":"Skills"},{"location":"about/#machine-deep-learning","text":"Related Theories and Algorithms: Probability, Statistics, Optimization, etc. Multi-layer Perceptron Convolutional Neural Networks (CNN) Recurrent Neural Networks (RNN) Generative Models (GANs, VAEs)","title":"Machine &amp; Deep Learning"},{"location":"about/#deep-learning-frameworks-order-of-frequently-used-frameworks","text":"PyTorch TensorFlow / Keras CNTK MXNet with Gluon","title":"Deep Learning Frameworks (Order of Frequently Used Frameworks)"},{"location":"about/#data-science","text":"Python(pands)/R SQL/NoSQL Data Wrangling Data Analytics Data Mining Data Visualization (matplotlib, ggplot, plotly, bokeh, etc.)","title":"Data Science"},{"location":"about/#database-order-of-frequently-used-tools","text":"MongoDB PostresSQL MySQL Firebird","title":"Database (Order of Frequently Used Tools)"},{"location":"about/#programming-languages-order-of-skillful-languages","text":"C/C++ Python R JavaScript C# Matlab Java","title":"Programming Languages (Order of Skillful Languages)"},{"location":"about/#computer-vision","text":"General Image Processing Algorithms (filters, etc.) Conventional & Deep Learning based CV Theories & Algorithms Object Detection Algorithms: YOLO, SSD, R-CNN, Fast R-CNN, Faster R-CNN, Selective Search, FPN, etc. Tools: Detectron2, TensorFlow Object Detection Module Segmentation: Mask R-CNN, FCN, U-Net, etc. Mixed Reality / Augmented reality OpenCV (C++ / Python / C#)","title":"Computer Vision"},{"location":"about/#cloud-computing","text":"Amazon Web Services (AWS) Google Cloud Platform (GCP) Google Colab (Social Coding)","title":"Cloud Computing"},{"location":"about/#web-development","text":"Basic Web Document Concepts: HTML5 / CSS / JavaScript Server-side: Node.jS, PHP Client-side: ReactJS, AngularJS, jQuery Other Web Technologies: WebGL / WebRTC / WebSocket Web Frameworks: React","title":"Web Development"},{"location":"about/#scientific-computing","text":"CUDA Parallel Computing Realtime Fluid Simulation Finite Element Analysis (FEA) Boundary Element Analaysis (BEA)","title":"Scientific Computing"},{"location":"about/#computer-graphics","text":"General Computer Graphics Theroies & Algorithms Rendering Theories: Global Illumination, Path Tracing, Ray Tracing Particle Simulation Tools: Maxon Cinema 4D, Adobe Photoshop & AfterEffects, NextLimit RealFlow, SideEffects Houdini, E-on Software Vue","title":"Computer Graphics"},{"location":"about/#publications","text":"","title":"Publications"},{"location":"about/#doctoral-thesis","text":"Isogeometric Analysis Based on Boundary Element Method , 2010, Thesis Ph.D., KAIST","title":"Doctoral Thesis"},{"location":"about/#international-conferences","text":"Application of Isogeometric Analysis to Shape Optimal Design of Thermal Problems, 8th World Congress on Structural and Multidisciplinary Optimization, Lisbon, Portugal, 2009. Structural shape optimization based on isogeometric analysis with control weights, International Congress of Theoretical and Applied Mechanics, Adelaide, Australia, 2008. Structural shape optimization using shape design sensitivity of NURBS control weights, CJK-OSM 5, Cheju, Korea, 2008. Structural shape optimization using extended finite element method on boundary representation by NURBS, 7th World Congress on Structural and Multidisciplinary Optimization, Seoul, Korea, 2007. Imposing essential and natural boundary conditions on geometric boundaries for X-FEM, CJK-OSM 4, Kumming, China, 2006.","title":"International Conferences"},{"location":"about/#student-symposium","text":"Bone growth simulation based on topology optimization, KAIST-Michigan Univ. Joint Symposium, University of Michigan, Ann Arbor, USA, 2006.","title":"Student Symposium"},{"location":"about/#articles","text":"Realtime Fluid Simulation Using Massively Parallel Computing with CUDA, 2013 , CJ POWERCAST Research Aritcles","title":"Articles"},{"location":"posts/deep-learning/object-detection-using-fast-rcnn/","text":"\uc774\ubc88 \ud3ec\uc2a4\ud305\uc5d0\uc11c\ub294 Object Detection\uc758 \ucd08\uae30 \uc5b4\ud50c\ub9ac\ucf00\uc774\uc158\uc778 R-CNN\uc758 \uc18d\ub3c4\uc640 \uc815\ud655\ub3c4\ub97c \uac1c\uc120\ud55c Fast R-CNN\uc5d0 \ub300\ud558\uc5ec \uac04\ub2e8\ud558\uac8c \uc54c\uc544\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. R-CNN\uc758 \ub2e8\uc810 \uadf8\ub7ec\uba74 \uc6b0\uc120 \uae30\uc874\uc758 R-CNN\uc774 \uac00\uc9c0\uace0 \uc788\ub294 \ub2e8\uc810\uc774 \ubb34\uc5c7\uc778\uc9c0 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. R-CNN\uc5d0\uc11c\ub294 \ud558\ub098\uc758 \uc774\ubbf8\uc9c0\uc5d0\uc11c Selective Search\ub97c \ud1b5\ud558\uc5ec \ucd5c\ub300 2,000\uac1c\uc758 Region Proposals\ub97c \uc0dd\uc131\ud558\uace0 \uac01\uac01\uc5d0 \ub300\ud558\uc5ec ConvNet(\uac00\ub839, AlexNet)\uc5d0 \ub300\ud55c Forward Pass\ub97c \uc5f0\uc0b0\ud569\ub2c8\ub2e4. \uc774\ub294 \ud559\uc2b5\uc2dc\uac04\ub3c4 \uc0c1\ub2f9\ud55c \uc2dc\uac04\uc774 \uc694\uad6c\ub418\uc9c0\ub9cc Runtime\uc5d0\uc11c\ub294 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uc218\uc900\uc774 \ub418\uc9c0 \ubabb\ud569\ub2c8\ub2e4. R-CNN\uc5d0\uc11c\ub294 \uc138 \uac1c\uc758 \ubaa8\ub378\uc744 \ubcc4\ub3c4\ub85d \ud559\uc2b5\uc2dc\ud0b5\ub2c8\ub2e4: Image Features\ub97c \ucd94\ucd9c\ud558\uae30 \uc704\ud55c CNN Model \uc774\ubbf8\uc9c0\ub97c \ubd84\ub958\ud558\uae30 \uc704\ud55c Classifier \ubc14\uc6b4\ub529 \ubc15\uc2a4\ub97c \uc2e4\uc81c \uc624\ube0c\uc81d\ud2b8 \uc704\uce58\uc5d0 \uc815\ud569\ud558\uae30 \uc704\ud55c Bounding Regressor \uadf8\ub7f0\ub370, \uc774 3\uac1c\uc758 \ubaa8\ub378\uc744 \ubcc4\ub3c4\ub85c \ud559\uc2b5\uc2dc\ud0a4\ub294 \uac83\uc740 \ud6a8\uc728\uc801\uc774\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. Fast R-CNN\uc758 \ub4f1\uc7a5 \uc704\uc5d0 \uc5b8\uae09\ub41c R-CNN\uc758 \ub2e8\uc810\uc744 \uadf9\ubcf5\ud558\uae30 \uc704\ud574 R-CNN \ub17c\ubb38\uc758 \uc800\uc790\uc778 Roos Girshick\uc740 2015\ub144\uc5d0 \uc544\ub798\uc640 \uac19\uc740 \uc0c8\ub85c\uc6b4 \ub17c\ubb38\uc744 \ubc1c\ud45c\ud569\ub2c8\ub2e4: Fast R-CNN, Ross Girshick, 2015 \uc704 \ub17c\ubb38\uc758 2\uac00\uc9c0 \ud575\uc2ec \uc544\uc774\ub514\uc5b4 \ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. Idea 1: RoI Pooling R-CNN\uc740 \uc774\ubbf8\uc9c0 \uc0c1\uc5d0\uc11c Region Proposals\ub97c \uc0dd\uc131 \uc2dc, \uc774\ub4e4\uc758 \uc601\uc5ed\uc774 \uc11c\ub85c \uacb9\uce58\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc544 \ubd88\ud544\uc694\ud558\uac8c CNN\uc758 \ubc18\ubcf5\uc801\uc778 Forward \uacc4\uc0b0(\ucd5c\ub300 2,000\ubc88)\uc73c\ub85c \uacc4\uc0b0 \ud6a8\uc728\uc131\uc744 \ub5a8\uc5b4\ub728\ub9bd\ub2c8\ub2e4. Ross\uac00 \uc81c\uc548\ud55c \uc544\uc774\ub514\uc5b4\ub294 \uc774\ubbf8\uc9c0 \uc0c1\uc774 \uc544\ub2cc CNN\uc758 Feature Map \uc0c1\uc5d0\uc11c RoI(Region of Interest)\ub97c \ucd94\ucd9c\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc989, CNN\uc758 Forward Pass\ub294 \ud558\ub098\uc758 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud558\uc5ec \ub531 \ud55c \ubc88\ub9cc \uc218\ud589\ud558\uc5ec Feature Map\uc744 \uc5bb\uace0, \ucd5c\ub300 2,000\uac1c\uc758 Region Proposals \uac04\uc5d0 Fearue Map\uc744 \uc11c\ub85c \uacf5\uc720\ud560 \uc218 \uc788\ub3c4\ub85d \ud558\ub294 \uac83\uc785\ub2c8\ub2e4. RoI Pooling, Image from Stanford\u2019s CS231N \uc2ac\ub77c\uc774\ub4dc \uc704\uc758 \uc774\ubbf8\uc9c0\ub294 \uc785\ub825 \uc774\ubbf8\uc9c0\ub85c\ubd80\ud130 CNN\uc5d0 \ub300\ud558\uc5ec Forward Pass\ud558\uc5ec Conv Feature Map\uc744 \uc5bb\uace0, Region Proposal \uc601\uc5ed\uc744 Feature Map \uc0c1\uc5d0 \ud22c\uc601(Projection)\ud55c \ud6c4, \uc774\ub97c \uc815\ud574\uc9c4 \ucc28\uc6d0(Dimension, \uac00\ub85c-\uc138\ub85c \ud574\uc0c1\ub3c4)\ub85c \uc601\uc5ed\uc744 \ubd84\ud560\ud569\ub2c8\ub2e4. \uac00\ub839, 3 \u00d7 800 \u00d7 600(\ucc44\ub110 \uc218 = 3, \ub192\uc774 = 800, \ub113\uc774 = 600)\uc758 \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c CNN\uc5d0 Forward Pass\ud558\uc5ec \uc5bb\ub294 Conv Feature Map\uc758 \uc0c1\uc5d0 \ud22c\uc601\ub41c Region Proposal \uc601\uc5ed\uc758 \ucc28\uc6d0\uc774 \\(C \u00d7 h \u00d7 w\\) \ub77c\uace0 \ud558\uba74, \uc774 \uc601\uc5ed\uc744 \uac01 \ucc44\ub110\uc5d0 \ub300\ud558\uc5ec \\(H \u00d7 W\\) \uac1c\uc218\ub85c \ubd84\ud560\ud569\ub2c8\ub2e4( \\(H\\) \uc640 \\(W\\) \ub97c Layer Hyper-parameters\ub77c\uace0 \ud569\ub2c8\ub2e4). RoI Max-pooling\uc740 \\(h \u00d7 w\\) \ud06c\uae30\uc758 RoI Windows\ub97c \\(H \u00d7 W\\) \uac1c\uc218\ub9cc\ud07c\uc758 Sub-window\ub85c \ubd84\ud560\ud558\ubbc0\ub85c \uac01 Sub-window\uc758 \ud06c\uae30\ub294 \ub300\ub7b5 \\(h/H \u00d7 w/W\\) \uc774 \ub418\uba70, \uac01 Sub-window \ub0b4\uc758 \uac12\ub4e4\uc744 Max-pooling\ud558\uc5ec Grid Cell\ub85c \ucd9c\ub825\ud569\ub2c8\ub2e4. Idea 2: \ud558\ub098\uc758 \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c \ubaa8\ub450 \ud2b8\ub808\uc774\ub2dd Fast R-CNN\uc758 \ub450\ubc88\uc9f8 \ud575\uc2ec \uc544\uc774\ub514\uc5b4\ub294, (1) Feature \ud559\uc2b5\uc744 \uc704\ud55c CNN, (2) \ubd84\ub958\ub97c \uc704\ud55c Classifier(SVM), (3) \ubc14\uc6b4\ub529 \ubc15\uc2a4\ub97c \uc624\ube0c\uc81d\ud2b8\uc5d0 \uc815\ud569\uc2dc\ud0a4\ub294 Regression (Linear Regressor)\uc744 \uac01\uac01 \ud2b8\ub808\uc774\ub2dd\ud558\ub294 R-CNN\uacfc\ub294 \ub2ec\ub9ac, Fast R-CNN\uc740 \uc774 \uc138\uac00\uc9c0\ub97c \ud558\ub098\uc758 \ub124\ud2b8\uc6cc\ud06c \ub0b4\uc5d0\uc11c \ud559\uc2b5\uc2dc\ud0b5\ub2c8\ub2e4. Fast R-CNN \uc544\ud0a4\ud14d\uccd0 \ubc0f \uad6c\uc870 \uc704\uc758 \uc774\ubbf8\uc9c0\ub97c \uc790\uc138\ud558\uac8c \uc0b4\ud3b4\ubcf4\uba74 \ub2e4\uc74c\uacfc \uc694\uc57d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c CNN\uc5d0 \ub300\ud558\uc5ec Forward Pass\ud558\uc5ec Conv Feature Map\uc744 \uc5bb\uace0, Max-pooling Layer\ub97c \uc5bb\uc2b5\ub2c8\ub2e4. \ud504\ub85c\uc81d\uc158 \ub41c \uac01\uac01\uc758 RoI\uc5d0 \ub300\ud558\uc5ec Pooling Layer\ub294 Conv Feature Map\uc73c\ub85c\ubd80\ud130 \uace0\uc815\ub41c \ud06c\uae30\uc758 Feature Vector\ub97c \ucd94\ucd9c\ud569\ub2c8\ub2e4. \uac01\uac01\uc758 Feature Vector\ub294 \uc77c\ub828\uc758 Fully Connected(FC) Layer\ub97c \uac70\uccd0\uc11c \\(K\\) \uac1c\uc758 \ud074\ub798\uc2a4 + \ubc30\uacbd \ud074\ub798\uc2a4\uc5d0 \ub300\ud55c Softmax \ud655\ub960\uc744 \uad6c\ud569\ub2c8\ub2e4. \uac01\uac01\uc758 \\(K\\) \uac1c\uc758 \uc624\ube0c\uc81d\ud2b8 \ud074\ub798\uc2a4\uc5d0 \ub300\ud55c 4\uac1c\uc758 \ubcc0\uc218 - \\((r,c,w,h)\\) - \ubc14\uc6b4\ub529 \ubc15\uc2a4\uc758 \uc67c\ucabd \uc0c1\ub2e8 \ubaa8\uc11c\ub9ac \uc88c\ud45c \\((r,c)\\) \ubc0f \ud06c\uae30 \\((w,h)\\) - \ub97c \ucd9c\ub825\ud569\ub2c8\ub2e4. \uc9c0\uae08\uae4c\uc9c0 Fast R-CNN\uc744 \uc774\uc6a9\ud55c Object Detection\uc5d0 \ub300\ud558\uc5ec \uc54c\uc544\ubcf4\uc558\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \ud3ec\uc2a4\ud305\uc5d0\uc11c\ub294 Faster R-CNN\uc744 \uc774\uc6a9\ud55c Object Detection\uc5d0 \ub300\ud558\uc5ec \uc54c\uc544\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","title":"Object Detection Using Fast R-CNN"},{"location":"posts/deep-learning/object-detection-using-fast-rcnn/#r-cnn","text":"\uadf8\ub7ec\uba74 \uc6b0\uc120 \uae30\uc874\uc758 R-CNN\uc774 \uac00\uc9c0\uace0 \uc788\ub294 \ub2e8\uc810\uc774 \ubb34\uc5c7\uc778\uc9c0 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. R-CNN\uc5d0\uc11c\ub294 \ud558\ub098\uc758 \uc774\ubbf8\uc9c0\uc5d0\uc11c Selective Search\ub97c \ud1b5\ud558\uc5ec \ucd5c\ub300 2,000\uac1c\uc758 Region Proposals\ub97c \uc0dd\uc131\ud558\uace0 \uac01\uac01\uc5d0 \ub300\ud558\uc5ec ConvNet(\uac00\ub839, AlexNet)\uc5d0 \ub300\ud55c Forward Pass\ub97c \uc5f0\uc0b0\ud569\ub2c8\ub2e4. \uc774\ub294 \ud559\uc2b5\uc2dc\uac04\ub3c4 \uc0c1\ub2f9\ud55c \uc2dc\uac04\uc774 \uc694\uad6c\ub418\uc9c0\ub9cc Runtime\uc5d0\uc11c\ub294 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uc218\uc900\uc774 \ub418\uc9c0 \ubabb\ud569\ub2c8\ub2e4. R-CNN\uc5d0\uc11c\ub294 \uc138 \uac1c\uc758 \ubaa8\ub378\uc744 \ubcc4\ub3c4\ub85d \ud559\uc2b5\uc2dc\ud0b5\ub2c8\ub2e4: Image Features\ub97c \ucd94\ucd9c\ud558\uae30 \uc704\ud55c CNN Model \uc774\ubbf8\uc9c0\ub97c \ubd84\ub958\ud558\uae30 \uc704\ud55c Classifier \ubc14\uc6b4\ub529 \ubc15\uc2a4\ub97c \uc2e4\uc81c \uc624\ube0c\uc81d\ud2b8 \uc704\uce58\uc5d0 \uc815\ud569\ud558\uae30 \uc704\ud55c Bounding Regressor \uadf8\ub7f0\ub370, \uc774 3\uac1c\uc758 \ubaa8\ub378\uc744 \ubcc4\ub3c4\ub85c \ud559\uc2b5\uc2dc\ud0a4\ub294 \uac83\uc740 \ud6a8\uc728\uc801\uc774\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.","title":"R-CNN\uc758 \ub2e8\uc810"},{"location":"posts/deep-learning/object-detection-using-fast-rcnn/#fast-r-cnn","text":"\uc704\uc5d0 \uc5b8\uae09\ub41c R-CNN\uc758 \ub2e8\uc810\uc744 \uadf9\ubcf5\ud558\uae30 \uc704\ud574 R-CNN \ub17c\ubb38\uc758 \uc800\uc790\uc778 Roos Girshick\uc740 2015\ub144\uc5d0 \uc544\ub798\uc640 \uac19\uc740 \uc0c8\ub85c\uc6b4 \ub17c\ubb38\uc744 \ubc1c\ud45c\ud569\ub2c8\ub2e4: Fast R-CNN, Ross Girshick, 2015 \uc704 \ub17c\ubb38\uc758 2\uac00\uc9c0 \ud575\uc2ec \uc544\uc774\ub514\uc5b4 \ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.","title":"Fast R-CNN\uc758 \ub4f1\uc7a5"},{"location":"posts/deep-learning/object-detection-using-fast-rcnn/#idea-1-roi-pooling","text":"R-CNN\uc740 \uc774\ubbf8\uc9c0 \uc0c1\uc5d0\uc11c Region Proposals\ub97c \uc0dd\uc131 \uc2dc, \uc774\ub4e4\uc758 \uc601\uc5ed\uc774 \uc11c\ub85c \uacb9\uce58\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc544 \ubd88\ud544\uc694\ud558\uac8c CNN\uc758 \ubc18\ubcf5\uc801\uc778 Forward \uacc4\uc0b0(\ucd5c\ub300 2,000\ubc88)\uc73c\ub85c \uacc4\uc0b0 \ud6a8\uc728\uc131\uc744 \ub5a8\uc5b4\ub728\ub9bd\ub2c8\ub2e4. Ross\uac00 \uc81c\uc548\ud55c \uc544\uc774\ub514\uc5b4\ub294 \uc774\ubbf8\uc9c0 \uc0c1\uc774 \uc544\ub2cc CNN\uc758 Feature Map \uc0c1\uc5d0\uc11c RoI(Region of Interest)\ub97c \ucd94\ucd9c\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc989, CNN\uc758 Forward Pass\ub294 \ud558\ub098\uc758 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud558\uc5ec \ub531 \ud55c \ubc88\ub9cc \uc218\ud589\ud558\uc5ec Feature Map\uc744 \uc5bb\uace0, \ucd5c\ub300 2,000\uac1c\uc758 Region Proposals \uac04\uc5d0 Fearue Map\uc744 \uc11c\ub85c \uacf5\uc720\ud560 \uc218 \uc788\ub3c4\ub85d \ud558\ub294 \uac83\uc785\ub2c8\ub2e4. RoI Pooling, Image from Stanford\u2019s CS231N \uc2ac\ub77c\uc774\ub4dc \uc704\uc758 \uc774\ubbf8\uc9c0\ub294 \uc785\ub825 \uc774\ubbf8\uc9c0\ub85c\ubd80\ud130 CNN\uc5d0 \ub300\ud558\uc5ec Forward Pass\ud558\uc5ec Conv Feature Map\uc744 \uc5bb\uace0, Region Proposal \uc601\uc5ed\uc744 Feature Map \uc0c1\uc5d0 \ud22c\uc601(Projection)\ud55c \ud6c4, \uc774\ub97c \uc815\ud574\uc9c4 \ucc28\uc6d0(Dimension, \uac00\ub85c-\uc138\ub85c \ud574\uc0c1\ub3c4)\ub85c \uc601\uc5ed\uc744 \ubd84\ud560\ud569\ub2c8\ub2e4. \uac00\ub839, 3 \u00d7 800 \u00d7 600(\ucc44\ub110 \uc218 = 3, \ub192\uc774 = 800, \ub113\uc774 = 600)\uc758 \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c CNN\uc5d0 Forward Pass\ud558\uc5ec \uc5bb\ub294 Conv Feature Map\uc758 \uc0c1\uc5d0 \ud22c\uc601\ub41c Region Proposal \uc601\uc5ed\uc758 \ucc28\uc6d0\uc774 \\(C \u00d7 h \u00d7 w\\) \ub77c\uace0 \ud558\uba74, \uc774 \uc601\uc5ed\uc744 \uac01 \ucc44\ub110\uc5d0 \ub300\ud558\uc5ec \\(H \u00d7 W\\) \uac1c\uc218\ub85c \ubd84\ud560\ud569\ub2c8\ub2e4( \\(H\\) \uc640 \\(W\\) \ub97c Layer Hyper-parameters\ub77c\uace0 \ud569\ub2c8\ub2e4). RoI Max-pooling\uc740 \\(h \u00d7 w\\) \ud06c\uae30\uc758 RoI Windows\ub97c \\(H \u00d7 W\\) \uac1c\uc218\ub9cc\ud07c\uc758 Sub-window\ub85c \ubd84\ud560\ud558\ubbc0\ub85c \uac01 Sub-window\uc758 \ud06c\uae30\ub294 \ub300\ub7b5 \\(h/H \u00d7 w/W\\) \uc774 \ub418\uba70, \uac01 Sub-window \ub0b4\uc758 \uac12\ub4e4\uc744 Max-pooling\ud558\uc5ec Grid Cell\ub85c \ucd9c\ub825\ud569\ub2c8\ub2e4.","title":"Idea 1: RoI Pooling"},{"location":"posts/deep-learning/object-detection-using-fast-rcnn/#idea-2","text":"Fast R-CNN\uc758 \ub450\ubc88\uc9f8 \ud575\uc2ec \uc544\uc774\ub514\uc5b4\ub294, (1) Feature \ud559\uc2b5\uc744 \uc704\ud55c CNN, (2) \ubd84\ub958\ub97c \uc704\ud55c Classifier(SVM), (3) \ubc14\uc6b4\ub529 \ubc15\uc2a4\ub97c \uc624\ube0c\uc81d\ud2b8\uc5d0 \uc815\ud569\uc2dc\ud0a4\ub294 Regression (Linear Regressor)\uc744 \uac01\uac01 \ud2b8\ub808\uc774\ub2dd\ud558\ub294 R-CNN\uacfc\ub294 \ub2ec\ub9ac, Fast R-CNN\uc740 \uc774 \uc138\uac00\uc9c0\ub97c \ud558\ub098\uc758 \ub124\ud2b8\uc6cc\ud06c \ub0b4\uc5d0\uc11c \ud559\uc2b5\uc2dc\ud0b5\ub2c8\ub2e4. Fast R-CNN \uc544\ud0a4\ud14d\uccd0 \ubc0f \uad6c\uc870 \uc704\uc758 \uc774\ubbf8\uc9c0\ub97c \uc790\uc138\ud558\uac8c \uc0b4\ud3b4\ubcf4\uba74 \ub2e4\uc74c\uacfc \uc694\uc57d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c CNN\uc5d0 \ub300\ud558\uc5ec Forward Pass\ud558\uc5ec Conv Feature Map\uc744 \uc5bb\uace0, Max-pooling Layer\ub97c \uc5bb\uc2b5\ub2c8\ub2e4. \ud504\ub85c\uc81d\uc158 \ub41c \uac01\uac01\uc758 RoI\uc5d0 \ub300\ud558\uc5ec Pooling Layer\ub294 Conv Feature Map\uc73c\ub85c\ubd80\ud130 \uace0\uc815\ub41c \ud06c\uae30\uc758 Feature Vector\ub97c \ucd94\ucd9c\ud569\ub2c8\ub2e4. \uac01\uac01\uc758 Feature Vector\ub294 \uc77c\ub828\uc758 Fully Connected(FC) Layer\ub97c \uac70\uccd0\uc11c \\(K\\) \uac1c\uc758 \ud074\ub798\uc2a4 + \ubc30\uacbd \ud074\ub798\uc2a4\uc5d0 \ub300\ud55c Softmax \ud655\ub960\uc744 \uad6c\ud569\ub2c8\ub2e4. \uac01\uac01\uc758 \\(K\\) \uac1c\uc758 \uc624\ube0c\uc81d\ud2b8 \ud074\ub798\uc2a4\uc5d0 \ub300\ud55c 4\uac1c\uc758 \ubcc0\uc218 - \\((r,c,w,h)\\) - \ubc14\uc6b4\ub529 \ubc15\uc2a4\uc758 \uc67c\ucabd \uc0c1\ub2e8 \ubaa8\uc11c\ub9ac \uc88c\ud45c \\((r,c)\\) \ubc0f \ud06c\uae30 \\((w,h)\\) - \ub97c \ucd9c\ub825\ud569\ub2c8\ub2e4. \uc9c0\uae08\uae4c\uc9c0 Fast R-CNN\uc744 \uc774\uc6a9\ud55c Object Detection\uc5d0 \ub300\ud558\uc5ec \uc54c\uc544\ubcf4\uc558\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \ud3ec\uc2a4\ud305\uc5d0\uc11c\ub294 Faster R-CNN\uc744 \uc774\uc6a9\ud55c Object Detection\uc5d0 \ub300\ud558\uc5ec \uc54c\uc544\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","title":"Idea 2: \ud558\ub098\uc758 \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c \ubaa8\ub450 \ud2b8\ub808\uc774\ub2dd"},{"location":"posts/deep-learning/object-detection-using-rcnn/","text":"\uc774\ubc88 \ud3ec\uc2a4\ud305\uc5d0\uc11c\ub294 Object Detection\uc758 \ucd08\uae30 \uc5b4\ud50c\ub9ac\ucf00\uc774\uc158\uc778 R-CNN\uc5d0 \ub300\ud558\uc5ec \uac04\ub2e8\ud788 \uc54c\uc544\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. Introduction R-CNN\uc740 Regional Convolutional Neural Networks\ub97c \uc758\ubbf8\ud558\ub294 \uac83\uc73c\ub85c, Jitendra Malik \uad50\uc218\uac00 \uc774\ub044\ub294 UC Berkley\uc758 \uc5f0\uad6c\uc790\ub4e4(Ross Girshick, Jeff Donahue, Trevor Darrell)\uc5d0 \uc758\ud574 \uc5f0\uad6c \ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uad00\ub828 \ub17c\ubb38\uc740 \uc544\ub798 \ub9c1\ud06c\ub97c \ucc38\uace0\ud558\uc2dc\uba74 \ub418\uaca0\uc2b5\ub2c8\ub2e4: Rich feature hierarchies for accurate object detection and semantic segmentation, 2014 R-CNN\uc744 \uad6c\uccb4\uc801\uc73c\ub85c \uc124\uba85\ud558\uae30 \uc804\uc5d0, \uc6b0\uc120 Object Detection\uc774 \ubb34\uc5c7\uc778\uc9c0 \uc54c\uc544\ubcf4\uace0\uc790 \ud569\ub2c8\ub2e4. Object Detection\uc774\ub780, \uc8fc\uc5b4\uc9c4 \uc774\ubbf8\uc9c0 \ub0b4\uc5d0 \uc5b4\ub290 \uc601\uc5ed\uc5d0 \uc624\ube0c\uc81d\ud2b8\uac00 \uc874\uc7ac\ud558\ub294\uc9c0 \uc54c\uc544\ub0b4\uace0, \ub9cc\uc57d \ud574\ub2f9 \uc601\uc5ed\uc5d0 \uc624\ube0c\uc81d\ud2b8\uac00 \uc874\uc7ac\ud55c\ub2e4\uba74 \uadf8 \uc624\ube0c\uc81d\ud2b8\uc758 \uc885\ub958\uac00 \ubb34\uc5c7\uc778\uc9c0(Classfication) \uc54c\uc544\ub0b4\ub294 \ubc29\ubc95\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. Jitendra Malik \uad50\uc218 \ud300\uc740, 2012\ub144\uc5d0 ILSVRC \uc5d0\uc11c \uc6b0\uc2b9\ud55c Toronto \ub300\ud559\uad50\uc758 Hinton \uad50\uc218 \uc5f0\uad6c\ud300\uc758 Alex Krizhevsky\uac00 \ubc1c\ud45c\ud55c AlexNet \uc5d0 \uc601\uac10\uc744 \ubc1b\uace0 \"CNN\uc744 Object Detection\uc5d0 \ud65c\uc6a9\ud560 \uc218 \uc788\uc744\uae4c?\"\ub77c\ub294 \uc9c8\ubb38\uc73c\ub85c \uc5f0\uad6c\ub97c \uc2dc\uc791\ud588\ub2e4\uace0 \ud569\ub2c8\ub2e4. \uc774 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\uace0\uc790, Jitendra Malik \uad50\uc218 \ud300\uc740 R-CNN\uc744 \uace0\uc548\ud558\uc5ec \uc774\ub97c PASCAL VOC \ub370\uc774\ud130\uc138\ud2b8\uc5d0 \uc801\uc6a9\ud558\uc600\uc73c\uba70 CNN\uc774 Object Detection \uc601\uc5ed\uc5d0\uc11c\ub3c4 \uae30\uc874 HoG \ub4f1\uacfc \uac19\uc740 Feature\uc5d0 \ube44\ud574 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \ud655\uc778\ud558\uc600\uc2b5\ub2c8\ub2e4. R-CNN \uc774\ud574\ud558\uae30 \uadf8\ub7fc \ubcf8\uaca9\uc801\uc73c\ub85c R-CNN\uc5d0 \ub300\ud558\uc5ec \uc774\ud574\ud574 \ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. R-CNN\uc758 \uc785\ub825\uacfc \ucd9c\ub825\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4: * \uc785\ub825: \uc774\ubbf8\uc9c0 * \ucd9c\ub825: \ubc14\uc6b4\ub529 \ubc15\uc2a4\ub4e4(Bounding Boxes), \uac01 \ubc14\uc6b4\ub529 \ubc15\uc2a4 \ub0b4\uc758 \uc624\ube0c\uc81d\ud2b8\uc758 \uc885\ub958(Label) \uadf8\ub7ec\uba74, \ubc14\uc6b4\ub529 \ubc15\uc2a4\ub97c \uc5b4\ub5bb\uac8c \uacb0\uc815\ud560 \uc218 \uc788\uc744\uae4c\uc694? Region Proposal \uac00\uc7a5 \ub2e8\uc21c\ud558\uace0 \uc9c1\uad00\uc801\uc73c\ub85c \uc0dd\uac01\ud558\uba74, \ub2e4\uc591\ud55c \ud06c\uae30\uc640 \ub2e4\uc591\ud55c \uc885\ud6a1\ube44(Aspect Ratio)\ub97c \uac16\ub294 Windows\ub97c \uc124\uc815\ud558\uc5ec \uc774\ubbf8\uc9c0\ub97c \ucb48\uc6b1 \ud6d1\uc5b4\uac00\uba70 \uc624\ube0c\uc81d\ud2b8\uc758 \uc720\ubb34 \ubc0f \uc885\ub958\ub97c \uc54c\uc544\ub0b4\ub294 \ubc29\ubc95\uc774 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c Sliding Window \uae30\ubc95\uc774\ub77c\uace0 \ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub098, \uc774\ubbf8\uc9c0 \ub0b4\uc758 \uc624\ube0c\uc81d\ud2b8\uac00 \uc5b4\ub5a4 \ud06c\uae30\ub85c \uc5b4\ub5a4 \uc885\ud6a1\ube44\ub97c \uac16\ub294\uc9c0 \uc54c \uc218 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \ub9e4\uc6b0 \ub9ce\uc740 \uc885\ub958\uc758 WIndows\ub97c \uc0dd\uc131\ud574\uc57c \ud558\uae30 \ub54c\ubb38\uc5d0 \ub9e4\uc6b0 \ube44\ud6a8\uc728\uc801\uc778 \ubc29\ubc95\uc774\ub77c\uace0 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Sliding Window\ub97c \uc774\uc6a9\ud55c Object Detection \uc608\uc2dc - Image from pyimagesearch.com \uc624\ube0c\uc81d\ud2b8\uac00 \uc874\uc7ac\ud560 \uac83 \uac19\uc740 \uc601\uc5ed\uc744 \uc81c\uc548\ud558\ub294 \uac83\uc744 Region Proposals\ub77c\uace0 \ud558\ub294\ub370, Sliding Window\uc758 \uc774\ub7ec\ud55c \ube44\ud6a8\uc728\uc131 \ub54c\ubb38\uc5d0 \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 Region Proposals\ub85c\uc11c Selective Search \uae30\ubc95\uc744 \uc774\uc6a9\ud569\ub2c8\ub2e4. Selective Search\ub294 Object Detection\uc744 \uc704\ud55c Region Proposal \uc54c\uace0\ub9ac\uc998\uc73c\ub85c \ud53d\uc140 \uac04\uc758 \uceec\ub7ec, \ud14d\uc2a4\uccd0, \ud06c\uae30, \ubaa8\uc591 \ub4f1\uc758 \uc720\uc0ac\uc131\uc744 \uae30\ubc18\uc73c\ub85c \uc601\uc5ed\uc744 \ub113\ud600\uac00\uba70 Segnmentation(\ub610\ub294 Grouping)\ud558\ub294 \uae30\ubc95\uc785\ub2c8\ub2e4. Selective Search \uc608\uc2dc - Image from learnopencv.com R-CNN \uad6c\uc870 \ub17c\ubb38\uc5d0 \uc81c\uc2dc\ub41c R-CNN\uc758 \uad6c\uc870\ub294 \uc544\ub798 \uc774\ubbf8\uc9c0\uc640 \uac19\uc2b5\ub2c8\ub2e4. \uc6b0\uc120, \uc785\ub825 \uc774\ubbf8\uc9c0(Input Image)\ub85c\ubd80\ud130 \ucd5c\ub300 2000(~2k)\uac1c\uc758 Region Proposals\ub97c \ucd94\ucd9c\ud569\ub2c8\ub2e4. \uac01 \ucd94\ucd9c\ub41c Region\uc744 \uc815\ud574\uc9c4 \ud574\uc0c1\ub3c4\ub85c \ub9de\ucd94\ub294\ub370 \uc774\ub97c Warping\uc774\ub77c\uace0 \ud569\ub2c8\ub2e4. \uc774\ub294, CNN\uc774 \uc815\ud574\uc9c4 \ud574\uc0c1\ub3c4\ub85c \uc785\ub825\uc744 \ubc1b\uc544\uc57c \ud558\uae30 \ub54c\ubb38\uc5d0 \uac00\ub85c-\uc138\ub85c \ube44\uc728\uc744 \uc815\ud574\uc9c4 \uc785\ub825 \ud574\uc0c1\ub3c4\ub85c \ub9de\ucd94\ub294 \uacfc\uc815\uc5d0\uc11c \uc774\ubbf8\uc9c0\uc758 \ube44\uc728\uc744 \ube44\ud2b8\ub294(Warp) \uacfc\uc815\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. Warp\ub41c \uc601\uc5ed\uc744 AlexNet \ub4f1\uacfc \uac19\uc740 CNN\uc744 \ud1b5\ud574(\ub17c\ubb38\uc5d0\uc11c\ub294 AlexNet\uc744 \uc774\uc6a9\ud558\uc600\uc9c0\ub9cc, \uadf8 \ud6c4\uc5d0 \ubc1c\ud45c\ub41c ResNet, GoogLeNet \ub4f1\uc744 \uc774\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4) Feature Map\uc744 \uc5bb\uc2b5\ub2c8\ub2e4. \uc5bb\uc5b4\uc9c4 Feature Map\uc744 Support Vector Machine(SVM) \uc744 \uc774\uc6a9\ud558\uc5ec \uac01 Region\uc744 \ubd84\ub958\ud569\ub2c8\ub2e4. VOC 2007\uc758 Warp\ub41c \ud559\uc2b5 \uc0d8\ud50c\ub4e4 \ubc14\uc6b4\ub529 \ubc15\uc2a4 \uac1c\uc120 \ub9c8\uc9c0\ub9c9 \uc9c8\ubb38\uc740, \"\ubc14\uc6b4\ub529 \ubc15\uc2a4\ub97c \uc2e4\uc81c \uc624\ube0c\uc81d\ud2b8 \uc601\uc5ed\uc5d0 \ubc00\uc811\ud558\uac8c \ucc3e\uc544\ub0bc \uc218 \uc788\uc744 \uac83\uc778\uac00?\"\uc785\ub2c8\ub2e4. \uc774\ub294, Linear Regression \uae30\ubc95\uc744 \uc774\uc6a9\ud558\uc5ec \uac00\ub2a5\ud569\ub2c8\ub2e4. \ubc14\uc6b4\ub529 \ubc15\uc2a4\uc758 \ubcc0\uc218\ub97c \uc911\uc2ec\uc88c\ud45c, \ub113\uc774 \ubc0f \ub192\uc774\uc758 4\uac1c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: \\[ (C_x, C_y, W, H) \\] \uc5ec\uae30\uc11c, \\((C_x, C_y)\\) \ub294 \ubc14\uc6b4\ub529 \ubc15\uc2a4\uc758 \uc911\uc2ec\uc88c\ud45c\ub97c, \\(W\\) , \\(H\\) \ub294 \uac01\uac01 \ubc14\uc6b4\ub529 \ubc15\uc2a4\uc758 \ub112\uc774, \ub192\uc774\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4. \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294, \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 \\(N\\) \uac1c\uc758 \ubc14\uc6b4\ub529 \ubc15\uc2a4 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \uc774\uc6a9\ud569\ub2c8\ub2e4. \uc989, \\[ \\{ (P^i, G^i) \\}, i=1,...,N \\] \uc5ec\uae30\uc11c, \\(P^i\\) \ub294 \ubc14\uc6b4\ub529 \ubc15\uc2a4 Proposal\uc758 \\(i\\) \ubc88\uc9f8 \ub370\uc774\ud130\ub97c, \\(G^i\\) \ub294 Ground Truth\uc758 \\(i\\) \ubc88\uc9f8 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c, \\(P^i\\) \uac00 \\(G^i\\) \uc5d0 \uadfc\uc811\ud560 \uc218 \uc788\ub3c4\ub85d \uc801\uc808\ud55c Loss\ub97c \uc815\uc758\ud558\uc5ec \uc774\ub97c \ucd5c\uc18c\ud654\ud558\ub3c4\ub85d \ud559\uc2b5\uc744 \ud569\ub2c8\ub2e4. \uc774\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ub17c\ubb38\uc758 [C. Bounding-box regression] \ubd80\ubd84\uc744 \ucc38\uace0\ud558\uc2dc\uba74 \ub418\uaca0\uc2b5\ub2c8\ub2e4. \uc544\ub798 \uc774\ubbf8\uc9c0\ub4e4\uc740 R-CNN\uc744 \uc774\uc6a9\ud558\uc5ec Object Detection\ud55c \uacb0\uacfc\ub4e4\uc785\ub2c8\ub2e4. \uc694\uc57d \uc9c0\uae08\uae4c\uc9c0 R-CNN\uc5d0 \ub300\ud558\uc5ec \uc54c\uc544\ubcf4\uc558\uc2b5\ub2c8\ub2e4. R-CNN \ubc29\ubc95\uc744 \uc694\uc57d\ud558\uc790\uba74, \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \ubc1b\ub294\ub2e4. Selective Search\ub97c \uc774\uc6a9\ud558\uc5ec Region Proposals\ub97c \ucd5c\ub300 2,000\uac1c \uc0dd\uc131\ud55c\ub2e4. \uc0dd\uc131\ub41c \uac01 Region Proposal\uc744 Warping\ud55c \ud6c4, \uc774\ub97c CNN\uc744 \uc774\uc6a9\ud558\uc5ec Feature Map\uc744 \uc0dd\uc131\ud55c\ub2e4. \uc0dd\uc131\ub41c Feature Map\uc744 SVM\uc744 \uc774\uc6a9\ud558\uc5ec \ubd84\ub958\ud55c\ub2e4. \ubc14\uc6b4\ub529 \ubc15\uc2a4\uc5d0 \ub300\ud558\uc5ec Linear Regression(Bounding-box Regressor)\ub97c \ud1b5\ud574 \uc624\ube0c\uc81d\ud2b8\uc758 \uc704\uce58\uc5d0 \uadfc\uc811\ud558\ub3c4\ub85d \ub9de\ucd98\ub2e4. \ub2e4\uc74c \ud3ec\uc2a4\ud305\uc5d0\uc11c\ub294 R-CNN\uc758 \uc18d\ub3c4\ub97c \uac1c\uc120\ud55c Fast R-CNN\uc5d0 \ub300\ud558\uc5ec \uc54c\uc544\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","title":"Object Detection Using R-CNN"},{"location":"posts/deep-learning/object-detection-using-rcnn/#introduction","text":"R-CNN\uc740 Regional Convolutional Neural Networks\ub97c \uc758\ubbf8\ud558\ub294 \uac83\uc73c\ub85c, Jitendra Malik \uad50\uc218\uac00 \uc774\ub044\ub294 UC Berkley\uc758 \uc5f0\uad6c\uc790\ub4e4(Ross Girshick, Jeff Donahue, Trevor Darrell)\uc5d0 \uc758\ud574 \uc5f0\uad6c \ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uad00\ub828 \ub17c\ubb38\uc740 \uc544\ub798 \ub9c1\ud06c\ub97c \ucc38\uace0\ud558\uc2dc\uba74 \ub418\uaca0\uc2b5\ub2c8\ub2e4: Rich feature hierarchies for accurate object detection and semantic segmentation, 2014 R-CNN\uc744 \uad6c\uccb4\uc801\uc73c\ub85c \uc124\uba85\ud558\uae30 \uc804\uc5d0, \uc6b0\uc120 Object Detection\uc774 \ubb34\uc5c7\uc778\uc9c0 \uc54c\uc544\ubcf4\uace0\uc790 \ud569\ub2c8\ub2e4. Object Detection\uc774\ub780, \uc8fc\uc5b4\uc9c4 \uc774\ubbf8\uc9c0 \ub0b4\uc5d0 \uc5b4\ub290 \uc601\uc5ed\uc5d0 \uc624\ube0c\uc81d\ud2b8\uac00 \uc874\uc7ac\ud558\ub294\uc9c0 \uc54c\uc544\ub0b4\uace0, \ub9cc\uc57d \ud574\ub2f9 \uc601\uc5ed\uc5d0 \uc624\ube0c\uc81d\ud2b8\uac00 \uc874\uc7ac\ud55c\ub2e4\uba74 \uadf8 \uc624\ube0c\uc81d\ud2b8\uc758 \uc885\ub958\uac00 \ubb34\uc5c7\uc778\uc9c0(Classfication) \uc54c\uc544\ub0b4\ub294 \ubc29\ubc95\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. Jitendra Malik \uad50\uc218 \ud300\uc740, 2012\ub144\uc5d0 ILSVRC \uc5d0\uc11c \uc6b0\uc2b9\ud55c Toronto \ub300\ud559\uad50\uc758 Hinton \uad50\uc218 \uc5f0\uad6c\ud300\uc758 Alex Krizhevsky\uac00 \ubc1c\ud45c\ud55c AlexNet \uc5d0 \uc601\uac10\uc744 \ubc1b\uace0 \"CNN\uc744 Object Detection\uc5d0 \ud65c\uc6a9\ud560 \uc218 \uc788\uc744\uae4c?\"\ub77c\ub294 \uc9c8\ubb38\uc73c\ub85c \uc5f0\uad6c\ub97c \uc2dc\uc791\ud588\ub2e4\uace0 \ud569\ub2c8\ub2e4. \uc774 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\uace0\uc790, Jitendra Malik \uad50\uc218 \ud300\uc740 R-CNN\uc744 \uace0\uc548\ud558\uc5ec \uc774\ub97c PASCAL VOC \ub370\uc774\ud130\uc138\ud2b8\uc5d0 \uc801\uc6a9\ud558\uc600\uc73c\uba70 CNN\uc774 Object Detection \uc601\uc5ed\uc5d0\uc11c\ub3c4 \uae30\uc874 HoG \ub4f1\uacfc \uac19\uc740 Feature\uc5d0 \ube44\ud574 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \ud655\uc778\ud558\uc600\uc2b5\ub2c8\ub2e4.","title":"Introduction"},{"location":"posts/deep-learning/object-detection-using-rcnn/#r-cnn","text":"\uadf8\ub7fc \ubcf8\uaca9\uc801\uc73c\ub85c R-CNN\uc5d0 \ub300\ud558\uc5ec \uc774\ud574\ud574 \ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. R-CNN\uc758 \uc785\ub825\uacfc \ucd9c\ub825\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4: * \uc785\ub825: \uc774\ubbf8\uc9c0 * \ucd9c\ub825: \ubc14\uc6b4\ub529 \ubc15\uc2a4\ub4e4(Bounding Boxes), \uac01 \ubc14\uc6b4\ub529 \ubc15\uc2a4 \ub0b4\uc758 \uc624\ube0c\uc81d\ud2b8\uc758 \uc885\ub958(Label) \uadf8\ub7ec\uba74, \ubc14\uc6b4\ub529 \ubc15\uc2a4\ub97c \uc5b4\ub5bb\uac8c \uacb0\uc815\ud560 \uc218 \uc788\uc744\uae4c\uc694?","title":"R-CNN \uc774\ud574\ud558\uae30"},{"location":"posts/deep-learning/object-detection-using-rcnn/#region-proposal","text":"\uac00\uc7a5 \ub2e8\uc21c\ud558\uace0 \uc9c1\uad00\uc801\uc73c\ub85c \uc0dd\uac01\ud558\uba74, \ub2e4\uc591\ud55c \ud06c\uae30\uc640 \ub2e4\uc591\ud55c \uc885\ud6a1\ube44(Aspect Ratio)\ub97c \uac16\ub294 Windows\ub97c \uc124\uc815\ud558\uc5ec \uc774\ubbf8\uc9c0\ub97c \ucb48\uc6b1 \ud6d1\uc5b4\uac00\uba70 \uc624\ube0c\uc81d\ud2b8\uc758 \uc720\ubb34 \ubc0f \uc885\ub958\ub97c \uc54c\uc544\ub0b4\ub294 \ubc29\ubc95\uc774 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c Sliding Window \uae30\ubc95\uc774\ub77c\uace0 \ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub098, \uc774\ubbf8\uc9c0 \ub0b4\uc758 \uc624\ube0c\uc81d\ud2b8\uac00 \uc5b4\ub5a4 \ud06c\uae30\ub85c \uc5b4\ub5a4 \uc885\ud6a1\ube44\ub97c \uac16\ub294\uc9c0 \uc54c \uc218 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \ub9e4\uc6b0 \ub9ce\uc740 \uc885\ub958\uc758 WIndows\ub97c \uc0dd\uc131\ud574\uc57c \ud558\uae30 \ub54c\ubb38\uc5d0 \ub9e4\uc6b0 \ube44\ud6a8\uc728\uc801\uc778 \ubc29\ubc95\uc774\ub77c\uace0 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Sliding Window\ub97c \uc774\uc6a9\ud55c Object Detection \uc608\uc2dc - Image from pyimagesearch.com \uc624\ube0c\uc81d\ud2b8\uac00 \uc874\uc7ac\ud560 \uac83 \uac19\uc740 \uc601\uc5ed\uc744 \uc81c\uc548\ud558\ub294 \uac83\uc744 Region Proposals\ub77c\uace0 \ud558\ub294\ub370, Sliding Window\uc758 \uc774\ub7ec\ud55c \ube44\ud6a8\uc728\uc131 \ub54c\ubb38\uc5d0 \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 Region Proposals\ub85c\uc11c Selective Search \uae30\ubc95\uc744 \uc774\uc6a9\ud569\ub2c8\ub2e4. Selective Search\ub294 Object Detection\uc744 \uc704\ud55c Region Proposal \uc54c\uace0\ub9ac\uc998\uc73c\ub85c \ud53d\uc140 \uac04\uc758 \uceec\ub7ec, \ud14d\uc2a4\uccd0, \ud06c\uae30, \ubaa8\uc591 \ub4f1\uc758 \uc720\uc0ac\uc131\uc744 \uae30\ubc18\uc73c\ub85c \uc601\uc5ed\uc744 \ub113\ud600\uac00\uba70 Segnmentation(\ub610\ub294 Grouping)\ud558\ub294 \uae30\ubc95\uc785\ub2c8\ub2e4. Selective Search \uc608\uc2dc - Image from learnopencv.com","title":"Region Proposal"},{"location":"posts/deep-learning/object-detection-using-rcnn/#r-cnn_1","text":"\ub17c\ubb38\uc5d0 \uc81c\uc2dc\ub41c R-CNN\uc758 \uad6c\uc870\ub294 \uc544\ub798 \uc774\ubbf8\uc9c0\uc640 \uac19\uc2b5\ub2c8\ub2e4. \uc6b0\uc120, \uc785\ub825 \uc774\ubbf8\uc9c0(Input Image)\ub85c\ubd80\ud130 \ucd5c\ub300 2000(~2k)\uac1c\uc758 Region Proposals\ub97c \ucd94\ucd9c\ud569\ub2c8\ub2e4. \uac01 \ucd94\ucd9c\ub41c Region\uc744 \uc815\ud574\uc9c4 \ud574\uc0c1\ub3c4\ub85c \ub9de\ucd94\ub294\ub370 \uc774\ub97c Warping\uc774\ub77c\uace0 \ud569\ub2c8\ub2e4. \uc774\ub294, CNN\uc774 \uc815\ud574\uc9c4 \ud574\uc0c1\ub3c4\ub85c \uc785\ub825\uc744 \ubc1b\uc544\uc57c \ud558\uae30 \ub54c\ubb38\uc5d0 \uac00\ub85c-\uc138\ub85c \ube44\uc728\uc744 \uc815\ud574\uc9c4 \uc785\ub825 \ud574\uc0c1\ub3c4\ub85c \ub9de\ucd94\ub294 \uacfc\uc815\uc5d0\uc11c \uc774\ubbf8\uc9c0\uc758 \ube44\uc728\uc744 \ube44\ud2b8\ub294(Warp) \uacfc\uc815\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. Warp\ub41c \uc601\uc5ed\uc744 AlexNet \ub4f1\uacfc \uac19\uc740 CNN\uc744 \ud1b5\ud574(\ub17c\ubb38\uc5d0\uc11c\ub294 AlexNet\uc744 \uc774\uc6a9\ud558\uc600\uc9c0\ub9cc, \uadf8 \ud6c4\uc5d0 \ubc1c\ud45c\ub41c ResNet, GoogLeNet \ub4f1\uc744 \uc774\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4) Feature Map\uc744 \uc5bb\uc2b5\ub2c8\ub2e4. \uc5bb\uc5b4\uc9c4 Feature Map\uc744 Support Vector Machine(SVM) \uc744 \uc774\uc6a9\ud558\uc5ec \uac01 Region\uc744 \ubd84\ub958\ud569\ub2c8\ub2e4. VOC 2007\uc758 Warp\ub41c \ud559\uc2b5 \uc0d8\ud50c\ub4e4","title":"R-CNN \uad6c\uc870"},{"location":"posts/deep-learning/object-detection-using-rcnn/#_1","text":"\ub9c8\uc9c0\ub9c9 \uc9c8\ubb38\uc740, \"\ubc14\uc6b4\ub529 \ubc15\uc2a4\ub97c \uc2e4\uc81c \uc624\ube0c\uc81d\ud2b8 \uc601\uc5ed\uc5d0 \ubc00\uc811\ud558\uac8c \ucc3e\uc544\ub0bc \uc218 \uc788\uc744 \uac83\uc778\uac00?\"\uc785\ub2c8\ub2e4. \uc774\ub294, Linear Regression \uae30\ubc95\uc744 \uc774\uc6a9\ud558\uc5ec \uac00\ub2a5\ud569\ub2c8\ub2e4. \ubc14\uc6b4\ub529 \ubc15\uc2a4\uc758 \ubcc0\uc218\ub97c \uc911\uc2ec\uc88c\ud45c, \ub113\uc774 \ubc0f \ub192\uc774\uc758 4\uac1c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: \\[ (C_x, C_y, W, H) \\] \uc5ec\uae30\uc11c, \\((C_x, C_y)\\) \ub294 \ubc14\uc6b4\ub529 \ubc15\uc2a4\uc758 \uc911\uc2ec\uc88c\ud45c\ub97c, \\(W\\) , \\(H\\) \ub294 \uac01\uac01 \ubc14\uc6b4\ub529 \ubc15\uc2a4\uc758 \ub112\uc774, \ub192\uc774\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4. \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294, \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 \\(N\\) \uac1c\uc758 \ubc14\uc6b4\ub529 \ubc15\uc2a4 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \uc774\uc6a9\ud569\ub2c8\ub2e4. \uc989, \\[ \\{ (P^i, G^i) \\}, i=1,...,N \\] \uc5ec\uae30\uc11c, \\(P^i\\) \ub294 \ubc14\uc6b4\ub529 \ubc15\uc2a4 Proposal\uc758 \\(i\\) \ubc88\uc9f8 \ub370\uc774\ud130\ub97c, \\(G^i\\) \ub294 Ground Truth\uc758 \\(i\\) \ubc88\uc9f8 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c, \\(P^i\\) \uac00 \\(G^i\\) \uc5d0 \uadfc\uc811\ud560 \uc218 \uc788\ub3c4\ub85d \uc801\uc808\ud55c Loss\ub97c \uc815\uc758\ud558\uc5ec \uc774\ub97c \ucd5c\uc18c\ud654\ud558\ub3c4\ub85d \ud559\uc2b5\uc744 \ud569\ub2c8\ub2e4. \uc774\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ub17c\ubb38\uc758 [C. Bounding-box regression] \ubd80\ubd84\uc744 \ucc38\uace0\ud558\uc2dc\uba74 \ub418\uaca0\uc2b5\ub2c8\ub2e4. \uc544\ub798 \uc774\ubbf8\uc9c0\ub4e4\uc740 R-CNN\uc744 \uc774\uc6a9\ud558\uc5ec Object Detection\ud55c \uacb0\uacfc\ub4e4\uc785\ub2c8\ub2e4.","title":"\ubc14\uc6b4\ub529 \ubc15\uc2a4 \uac1c\uc120"},{"location":"posts/deep-learning/object-detection-using-rcnn/#_2","text":"\uc9c0\uae08\uae4c\uc9c0 R-CNN\uc5d0 \ub300\ud558\uc5ec \uc54c\uc544\ubcf4\uc558\uc2b5\ub2c8\ub2e4. R-CNN \ubc29\ubc95\uc744 \uc694\uc57d\ud558\uc790\uba74, \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \ubc1b\ub294\ub2e4. Selective Search\ub97c \uc774\uc6a9\ud558\uc5ec Region Proposals\ub97c \ucd5c\ub300 2,000\uac1c \uc0dd\uc131\ud55c\ub2e4. \uc0dd\uc131\ub41c \uac01 Region Proposal\uc744 Warping\ud55c \ud6c4, \uc774\ub97c CNN\uc744 \uc774\uc6a9\ud558\uc5ec Feature Map\uc744 \uc0dd\uc131\ud55c\ub2e4. \uc0dd\uc131\ub41c Feature Map\uc744 SVM\uc744 \uc774\uc6a9\ud558\uc5ec \ubd84\ub958\ud55c\ub2e4. \ubc14\uc6b4\ub529 \ubc15\uc2a4\uc5d0 \ub300\ud558\uc5ec Linear Regression(Bounding-box Regressor)\ub97c \ud1b5\ud574 \uc624\ube0c\uc81d\ud2b8\uc758 \uc704\uce58\uc5d0 \uadfc\uc811\ud558\ub3c4\ub85d \ub9de\ucd98\ub2e4. \ub2e4\uc74c \ud3ec\uc2a4\ud305\uc5d0\uc11c\ub294 R-CNN\uc758 \uc18d\ub3c4\ub97c \uac1c\uc120\ud55c Fast R-CNN\uc5d0 \ub300\ud558\uc5ec \uc54c\uc544\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","title":"\uc694\uc57d"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/","text":"Written by Alex Choi, Oct. 8, 2020. Basic Environment Anaconda Git Creating CONDA Environment Run a command line tool, i.e. Anaconda Prompt and type the following command: conda create -n centernet python=3.7 NOTE: Python 3.8 not tested yet. But you can try if you want. Activate centernet Anaconda environment: conda activate centernet Installing Python Packages Install pytorch(ver.1.4) and torchvision based on your cudatoolkit version. conda install pytorch=1.4 torchvision cudatoolkit=10.1 -c pytorch NOTE: CenterNet build didn't work with pytorch version 1.5 or 1.6. NOTE: cudatoolkit version 10.2 not tested yet. Install necessary python packages: python -m pip install opencv-python Cython numba progress matplotlib easydict scipy Building cocoapi Tools Clone cocoapi tools git repository to any path where you want git clone https://github.com/cocodataset/cocoapi.git Open cocoapi/PythonAPI/setup.py using a text editor and modify the following line: extra_compile_args=['-Wno-cpp', '-Wno-unused-function', '-std=c99'] to extra_compile_args={'gcc': ['/Qstd=c99']}, In the command line tool move to the following path: cd cocoapi/PythonAPI Build cocoapi tools: python setup.py build_ext install Modifying cpp_extension.py Using a text editor open up C:/ProgramData/Anaconda3/envs/centernet/Lib/site-packages/torch/utils/cpp_extension.py and modify the following line: match = re.search(r'(\\d+)\\.(\\d+)\\.(\\d+)', compiler_info.decode().strip()) to match = re.search(r'(\\d+)\\.(\\d+)\\.(\\d+)', compiler_info.decode(\"utf8\", \"ignore\").strip()) Copying and Unzipping CenterNet.zip Copy \"CenterNet.zip\" file to any path you want (maybe COI project folder). Unzip the file using BandiZip or any unzipping software you want. NOTE: You can find more information from the PyTorch CenterNet official repo . Building NMS 'nms' is short for \"Non-Maximum Suppression.\" CenterNet does not usually use Non-Maximum Suppression, but it is sometimes useful. In order to avoid build error comment out the following line after opening CenterNet/src/lib/external/setup.py up using a text editor. #extra_compile_args=[\"-Wno-cpp\", \"-Wno-unused-function\"] NOTE: The line above may be already commented out actually. Build nms with the following command line: python setup.py build_ext --inplace NOTE: You can find more information about building nms here . Building DCNv2 Move to the following path in your CenterNet path: cd CenterNet/src/lib/models/networks/DCNv2/ Build DCNv2 using the following command line (this may take some time to finish building): python setup.py build develop NOTE: You can find more information about DCNv2 here . Test with Pre-trained Models Unzip \"ImageNet-Weights.zip\" and copy all the contained files (ImageNet pre-trained models) to C:\\Users\\{WINDOWS_USER_ACCOUNT_NAME}\\.cache\\torch\\checkpoints\\ . Unzip \"Centernet-Models.zip\" and copy all the contained files to CenterNet/models/ . Finally, test with some COI images using the following command line: python demo.py ctdet --demo ../data/COI/images/image_0001.jpg --load_model ../exp/ctdet/COI/model_best.pth python demo.py ctdet --demo ../data/COI/images/image_0002.jpg --load_model ../exp/ctdet/COI/model_best.pth python demo.py ctdet --demo ../data/COI/images/image_0003.jpg --load_model ../exp/ctdet/COI/model_best.pth Simple Trial for Training If you are using CenterNet.zip you can find the training information for COI from CenterNet/exp/ctdet/COI/ . There are 2 options for training: One is training from scratch (random initialization). python main.py ctdet --exp_id COI --batch_size 16 --lr 1.25e-4 --gpus 0 --load_model ../models/ctdet_coco_resdcn18.pth --resume False The other is resume training from pre-trained model. python main.py ctdet --exp_id COI --batch_size 16 --lr 1.25e-4 --gpus 0 --load_model ../models/ctdet_coco_resdcn18.pth --resume True Checking Training Information You can check your training information such as loss history, accuracy history and so on using TensorBoard . You can find your training information for COI from the path, CenterNet/exp/ctdet/COI/ Install tensorboard using the following command line: python -m pip install tensorboard","title":"How to Set Up PyTorch CenterNet Development Environment"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#basic-environment","text":"Anaconda Git","title":"Basic Environment"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#creating-conda-environment","text":"Run a command line tool, i.e. Anaconda Prompt and type the following command: conda create -n centernet python=3.7 NOTE: Python 3.8 not tested yet. But you can try if you want. Activate centernet Anaconda environment: conda activate centernet","title":"Creating CONDA Environment"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#installing-python-packages","text":"Install pytorch(ver.1.4) and torchvision based on your cudatoolkit version. conda install pytorch=1.4 torchvision cudatoolkit=10.1 -c pytorch NOTE: CenterNet build didn't work with pytorch version 1.5 or 1.6. NOTE: cudatoolkit version 10.2 not tested yet. Install necessary python packages: python -m pip install opencv-python Cython numba progress matplotlib easydict scipy","title":"Installing Python Packages"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#building-cocoapi-tools","text":"Clone cocoapi tools git repository to any path where you want git clone https://github.com/cocodataset/cocoapi.git Open cocoapi/PythonAPI/setup.py using a text editor and modify the following line: extra_compile_args=['-Wno-cpp', '-Wno-unused-function', '-std=c99'] to extra_compile_args={'gcc': ['/Qstd=c99']}, In the command line tool move to the following path: cd cocoapi/PythonAPI Build cocoapi tools: python setup.py build_ext install","title":"Building cocoapi Tools"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#modifying-cpp_extensionpy","text":"Using a text editor open up C:/ProgramData/Anaconda3/envs/centernet/Lib/site-packages/torch/utils/cpp_extension.py and modify the following line: match = re.search(r'(\\d+)\\.(\\d+)\\.(\\d+)', compiler_info.decode().strip()) to match = re.search(r'(\\d+)\\.(\\d+)\\.(\\d+)', compiler_info.decode(\"utf8\", \"ignore\").strip())","title":"Modifying cpp_extension.py"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#copying-and-unzipping-centernetzip","text":"Copy \"CenterNet.zip\" file to any path you want (maybe COI project folder). Unzip the file using BandiZip or any unzipping software you want. NOTE: You can find more information from the PyTorch CenterNet official repo .","title":"Copying and Unzipping CenterNet.zip"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#building-nms","text":"'nms' is short for \"Non-Maximum Suppression.\" CenterNet does not usually use Non-Maximum Suppression, but it is sometimes useful. In order to avoid build error comment out the following line after opening CenterNet/src/lib/external/setup.py up using a text editor. #extra_compile_args=[\"-Wno-cpp\", \"-Wno-unused-function\"] NOTE: The line above may be already commented out actually. Build nms with the following command line: python setup.py build_ext --inplace NOTE: You can find more information about building nms here .","title":"Building NMS"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#building-dcnv2","text":"Move to the following path in your CenterNet path: cd CenterNet/src/lib/models/networks/DCNv2/ Build DCNv2 using the following command line (this may take some time to finish building): python setup.py build develop NOTE: You can find more information about DCNv2 here .","title":"Building DCNv2"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#test-with-pre-trained-models","text":"Unzip \"ImageNet-Weights.zip\" and copy all the contained files (ImageNet pre-trained models) to C:\\Users\\{WINDOWS_USER_ACCOUNT_NAME}\\.cache\\torch\\checkpoints\\ . Unzip \"Centernet-Models.zip\" and copy all the contained files to CenterNet/models/ . Finally, test with some COI images using the following command line: python demo.py ctdet --demo ../data/COI/images/image_0001.jpg --load_model ../exp/ctdet/COI/model_best.pth python demo.py ctdet --demo ../data/COI/images/image_0002.jpg --load_model ../exp/ctdet/COI/model_best.pth python demo.py ctdet --demo ../data/COI/images/image_0003.jpg --load_model ../exp/ctdet/COI/model_best.pth","title":"Test with Pre-trained Models"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#simple-trial-for-training","text":"If you are using CenterNet.zip you can find the training information for COI from CenterNet/exp/ctdet/COI/ . There are 2 options for training: One is training from scratch (random initialization). python main.py ctdet --exp_id COI --batch_size 16 --lr 1.25e-4 --gpus 0 --load_model ../models/ctdet_coco_resdcn18.pth --resume False The other is resume training from pre-trained model. python main.py ctdet --exp_id COI --batch_size 16 --lr 1.25e-4 --gpus 0 --load_model ../models/ctdet_coco_resdcn18.pth --resume True","title":"Simple Trial for Training"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#checking-training-information","text":"You can check your training information such as loss history, accuracy history and so on using TensorBoard . You can find your training information for COI from the path, CenterNet/exp/ctdet/COI/ Install tensorboard using the following command line: python -m pip install tensorboard","title":"Checking Training Information"},{"location":"posts/scientific-computing/realtime-fluid-simulation-using-cuda/","text":"This is one of our researches done in CJ POWERCAST, which is dedicated from the members of our team, T.J. Kwak and J.M. Park and G.Choi. INTORUDUCTION GPU (Graphics Process Unit) has been traditionally used only for display of graphical contents with graphics acceleration. Because of the characteristics of display that represent pixels as a massive array, memory and processing architectures of GPU are structured in parallel in order to process the massive graphical data in a short period. Some pioneers have used these features for parallel computation as well as displaying computer graphics. Consequently GPU manufacturers realized their needs of general purposes of GPU. FIG 1. Comparsion of Processor Structures: CPU VS GPU As shown in FIG 1., GPU has a huge number of ALUs, which are in charge of arithmetic operations, enabling massively parallel processing. With this background, GPGPU ( General Purpose Graphics Process Unit ) comes into the world and aims to process massively parallel computations not only for computer graphics but for general fields such as prediction of stock, weather, science, etc. Most of the simulations in computer graphics are based on natural phenomena and tend to need massive computations to mimic the real nature. Our research team has expected that GPGPU technology would be used for wider fields of our related research works. IMPLEMENTATION The goal of this research is, as a preceding research, to apply the GPU technology to a realtime fluid simulation that needs to solve the natural phonomenon called \"Navier-Stokes equation\" with parallel processing. To this end we adopted the algorhithms from the simplified version of Navier-Stokes equation( Jos Stam, Real-Time Fluid Dynamics for Games, 2003 Game Development Conference ) and modified the processing structures in order for GPUs to perform the massive calculation in parallel. The simplified Navier-Stokes equation for a realtime calculation used in this research looks like: \\[ \\frac{\\partial \\mathrm{u}}{\\partial t} = -(\\mathrm{u} \\cdot \\nabla) \\mathrm{u} + \\nu \\nabla^2 \\mathrm{u} + \\mathrm{f} \\] \\[ \\frac{\\partial \\rho}{\\partial t} = -(\\mathrm{u} \\cdot \\nabla) \\rho + \\kappa \\nabla^2 \\rho + S \\] The above equation describes that the state of fluid at a given instant of time is modeled as a velocity field: a function that assigns a velocity vector to every point in space. Following this equation we designed the processing structure from Jos Stam's algorithms. FIG 2. Fluid Simulation Processing Flow of CPU Sequential Processing and GPU Parallel Processing Since this simulation performs calculations on each point within a specific area, it is a good case to verify massively parallel computation performance by GPGPU. We implemented for two cases, one carries out computation by CPU and the other the same by GPU to compare between their performances. As a result the simulation from CPU shows a good performance if the resolution is less than 128 X 128. However, if not the case, the performance is very poor. RESULTS As you can see the results on the video below, the simulation on 512 x 512 resolution goes terribly slow since CPU does sequential computations while the GPU simulation shows a nice result on the same resolution. The performance table of frame rates by testing the same simulation between CPU and GPU is given. [TABLE 1. Comparison of results between from CPU and from GPU for realtime fluid simulation] Resolution 128 \u00d7128 256\u00d7256 512\u00d7128 Framerates from CPU Processing 60 fps 30 fps 0.7 fps Framerates from GPU Processing - - 60 fps The simulation on CPU is performed for three different resolutions while that on GPU only for one resolution, 512 \u00d7 512, which is sufficient to verify the performance since running on CPU is very slow on 512 \u00d7 512 resolution. The test is run with Intel Xeon CPU 3.07Ghz 12 cores, 12GB RAM and NVIDIA Quadro 5000. In conclusion this research inspires us with the potential of GPU usage for general computation purposes. By theses reasons we should keep up with GPGPU technology corresponding to real-time simulations with massive computation. Why? Because the technology world requires us more and more massive computations.","title":"Realtime Fluid Simulation Using CUDA"},{"location":"posts/scientific-computing/realtime-fluid-simulation-using-cuda/#intoruduction","text":"GPU (Graphics Process Unit) has been traditionally used only for display of graphical contents with graphics acceleration. Because of the characteristics of display that represent pixels as a massive array, memory and processing architectures of GPU are structured in parallel in order to process the massive graphical data in a short period. Some pioneers have used these features for parallel computation as well as displaying computer graphics. Consequently GPU manufacturers realized their needs of general purposes of GPU. FIG 1. Comparsion of Processor Structures: CPU VS GPU As shown in FIG 1., GPU has a huge number of ALUs, which are in charge of arithmetic operations, enabling massively parallel processing. With this background, GPGPU ( General Purpose Graphics Process Unit ) comes into the world and aims to process massively parallel computations not only for computer graphics but for general fields such as prediction of stock, weather, science, etc. Most of the simulations in computer graphics are based on natural phenomena and tend to need massive computations to mimic the real nature. Our research team has expected that GPGPU technology would be used for wider fields of our related research works.","title":"INTORUDUCTION"},{"location":"posts/scientific-computing/realtime-fluid-simulation-using-cuda/#implementation","text":"The goal of this research is, as a preceding research, to apply the GPU technology to a realtime fluid simulation that needs to solve the natural phonomenon called \"Navier-Stokes equation\" with parallel processing. To this end we adopted the algorhithms from the simplified version of Navier-Stokes equation( Jos Stam, Real-Time Fluid Dynamics for Games, 2003 Game Development Conference ) and modified the processing structures in order for GPUs to perform the massive calculation in parallel. The simplified Navier-Stokes equation for a realtime calculation used in this research looks like: \\[ \\frac{\\partial \\mathrm{u}}{\\partial t} = -(\\mathrm{u} \\cdot \\nabla) \\mathrm{u} + \\nu \\nabla^2 \\mathrm{u} + \\mathrm{f} \\] \\[ \\frac{\\partial \\rho}{\\partial t} = -(\\mathrm{u} \\cdot \\nabla) \\rho + \\kappa \\nabla^2 \\rho + S \\] The above equation describes that the state of fluid at a given instant of time is modeled as a velocity field: a function that assigns a velocity vector to every point in space. Following this equation we designed the processing structure from Jos Stam's algorithms. FIG 2. Fluid Simulation Processing Flow of CPU Sequential Processing and GPU Parallel Processing Since this simulation performs calculations on each point within a specific area, it is a good case to verify massively parallel computation performance by GPGPU. We implemented for two cases, one carries out computation by CPU and the other the same by GPU to compare between their performances. As a result the simulation from CPU shows a good performance if the resolution is less than 128 X 128. However, if not the case, the performance is very poor.","title":"IMPLEMENTATION"},{"location":"posts/scientific-computing/realtime-fluid-simulation-using-cuda/#results","text":"As you can see the results on the video below, the simulation on 512 x 512 resolution goes terribly slow since CPU does sequential computations while the GPU simulation shows a nice result on the same resolution. The performance table of frame rates by testing the same simulation between CPU and GPU is given. [TABLE 1. Comparison of results between from CPU and from GPU for realtime fluid simulation] Resolution 128 \u00d7128 256\u00d7256 512\u00d7128 Framerates from CPU Processing 60 fps 30 fps 0.7 fps Framerates from GPU Processing - - 60 fps The simulation on CPU is performed for three different resolutions while that on GPU only for one resolution, 512 \u00d7 512, which is sufficient to verify the performance since running on CPU is very slow on 512 \u00d7 512 resolution. The test is run with Intel Xeon CPU 3.07Ghz 12 cores, 12GB RAM and NVIDIA Quadro 5000. In conclusion this research inspires us with the potential of GPU usage for general computation purposes. By theses reasons we should keep up with GPGPU technology corresponding to real-time simulations with massive computation. Why? Because the technology world requires us more and more massive computations.","title":"RESULTS"}]}